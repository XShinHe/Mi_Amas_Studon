{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 style=\"text-align:right\">The Course of MS,&nbsp; Peking University</h4>\n",
    "***\n",
    "<p>\n",
    "<h1 style=\"text-align:center\"><font face=\"Times Roman\" size=6>Stochastic simulation and application</font></h1>\n",
    "<br>\n",
    "<h2 style=\"text-align:center\"><font face=\"Times Roman\">prof.&nbsp;Tiejun Li</font> </h2>\n",
    "<h2 style=\"text-align:center\"><font face=\"Times Roman\">Records of the first term, 2018.9-2018.12</font> </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Discuss about the method following. What can you say about this strategy?  \n",
    "Compute the matrix product\n",
    "$$ C = AB $$\n",
    "where $A\\in R^{m\\times n} $, $B\\in R^{n\\times p}$, and assume $n \\gg 1$.    \n",
    "When $n$ is huge, which is possible in many applications in big data, the following randomized matrix multiplication was proposed:  \n",
    "Given any probability distribution $\\{p_i\\}$, where $p_i > 0$ and $\\sum_{i=1}^n p_i = 1 $, randomly pick\n",
    "$K$ columns with the $i_m$-th column from $A$, $L^{(m)}$ ; and the $i_m$-th row from $B$, $R^{(m)}$ , according to\n",
    "$\\{p_i\\}$. Correspondingly define:  \n",
    "$$ L^{(m)} = \\frac{1}{\\sqrt{K p_{i_m}}} A_{\\cdot,i_m} \\\\\n",
    "R^{(m)} = \\frac{1}{\\sqrt{K p_{i_m}}} B_{i_m, \\cdot} \\\\\n",
    "m = 1,\\cdots, K\n",
    "$$\n",
    "then compute\n",
    "$$ C \\approx \\sum_{m=1}^K L^{(m)} R^{(m)} $$\n",
    "Does it work? Is it possible to generalize and improve it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer:  \n",
    "> * Does it work?  \n",
    "its easy to proof \n",
    "$$ \\hat{C} = \\sum_{m=1}^K L^{(m)} R^{(m)} = \\sum_{m=1}^K \\frac{1}{K p_{i_m}} A_{:,i_m} B_{i_m, :} $$\n",
    "is unbiased estimate of multiplication of $AB$, for:  \n",
    "$$ \\mathbb{E}[\\hat{C}]  = \\sum_{m=1}^K \\sum_k \\mathbb{P}(i_m = k) \\frac{1}{K p_{k}} A_{;,k}B_{k,:} \\\\\n",
    " = \\sum_k A_{;,k}B_{k,:} = AB $$  \n",
    "\n",
    "> * to imporve it?  \n",
    "Abviously, approximation error (e.g. $ \\|AB âˆ’ \\hat{C}\\|$) depends on $\\{p_i\\}$. So by minimize $\\mathbb{E}[\\|\\hat{C}-AB\\|^2_F]$ as:  \n",
    "$$ \\mathbb{E}[\\|\\hat{C}-AB\\|^2_F] = \\mathbb{E}\\big[ \\sum_{i,j} (\\hat{C}_{i,j} - A_{i,:}B_{:,j} )^2\\big] \\\\\n",
    "= \\sum_{i,j} Var[\\hat{C}_{i,j}] = \\frac{1}{K}\\sum_{m=1}^K \\sum_{i,j} \\frac{A_{i,m}^2 B_{m,j}^2}{p_m} - \\frac{1}{K}\\sum_{i,j}(A_{i,:}B_{:,j})^2 \\\\\n",
    "= \\frac{1}{K}\\sum_{m=1}^K\\frac{1}{p_m}\\|A_{:,m}\\|_2^2\\|B_{m,:}\\|_2^2 - \\frac{1}{K}\\|AB\\|^2_F \\\\\n",
    "\\ge \\frac{1}{K}\\sum_{m=1}^K (\\|A_{;,m}\\|_2\\|B_{m,:}\\|_2)^2 - \\frac{1}{K}\\|AB\\|^2_F$$\n",
    "the last step is by Cauchy-Schwarz inequation, and when $p_i \\propto \\|A_{:,i}\\|_2\\|B_{i,:}\\|_2 $, or that:  \n",
    "$$p_i = \\frac{\\|A_{:,i}\\|_2\\|B_{i,:}\\|_2}{\\sum_m \\|A_{:,m}\\|_2\\|B_{m,:}\\|_2}$$\n",
    "yields its lower bound, or so called **optimal sampling probabilites**.  \n",
    "this proof refers [randomized linear algebra lecture of princeton](http://www.princeton.edu/~yc5/orf570/randomized_linear_algebra.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numerically testify the half order convergence of Monte Carlo integration  \n",
    "for\n",
    "$$ I(f) = \\int_0^1 sin x dx = \\mathbb{E} [ sin X ] $$\n",
    "where $X$ is uniformly distributed in $[0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we define $\\varepsilon = |I(f)-\\mathbb{E}[sinX]|$, so here shows numerical result:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.68648491 -0.5392868  -0.68159077 -0.45907247 -0.55658673 -0.67607233\n",
      " -0.67923463 -0.66493214 -0.4269418  -0.64370733 -0.52079557 -0.37686482\n",
      " -0.67500044 -0.5406533  -0.68472231 -0.29879397 -0.39516732 -0.50201626\n",
      " -0.44436686 -0.49271481 -0.38154628 -0.07461918 -0.16956485 -0.5477943\n",
      " -0.63138155 -0.63078915 -0.51415223 -0.44250829 -0.53606372 -0.46701099\n",
      " -0.55948649 -0.57067488]\n",
      "So note Err ~ N^(k),\n",
      "where <k> =  -0.5147061714991751 , Var(k) =  0.14528396119972328\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "exact = np.cos(0)-np.cos(1)\n",
    "Ntimes = 32\n",
    "ks = np.zeros(Ntimes)\n",
    "for j in range(Ntimes):\n",
    "    Nmax = 10000\n",
    "    N = np.arange(1,Nmax+1)\n",
    "    Xs = np.sin( np.random.uniform(0,1,Nmax) )\n",
    "    E = np.zeros(Nmax)\n",
    "    \n",
    "    E[0] = Xs[0]\n",
    "    for i in range(1,Nmax):\n",
    "        E[i] = (E[i-1]*i+Xs[i])/(i+1)\n",
    "\n",
    "    x = np.log(N[0:-1:1000])\n",
    "    y = np.log(np.abs(E[0:-1:1000]-exact))\n",
    "    A = np.vstack([x, np.ones(len(x))]).T\n",
    "\n",
    "    m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "    ks[j]= m\n",
    "    #plt.plot(x, y, 'r-.', label=r'orignal curve: $log|Err| \\sim logN$')\n",
    "    #plt.plot(x, m*x+c, 'b-', label='fitting curve: \\nlog|Err|=%.5f logN+%.5f'%(m,c))\n",
    "    #plt.legend(loc=1)\n",
    "    #plt.show()\n",
    "print(ks)\n",
    "print('So note Err ~ N^(k),\\nwhere <k> = ',ks.mean(),', Var(k) = ',ks.std())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
